# -*- coding: utf-8 -*-
"""AA_XAI_PArt 2_CNN_OCC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-KjLH01RzqUIHPsJYOOIrkimYCDs549
"""

import torch
import torch.optim as optim
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import pandas as pd
import os
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix

from google.colab import drive
import os
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Define the function to create dataset
def create_dataset(folder_path):
    my_list = []
    for category in ['NORMAL', 'PNEUMONIA']:  # Iterate over the two classes
        category_path = os.path.join(folder_path, category)
        if os.path.exists(category_path):  # Ensure the category folder exists
            for file_name in os.listdir(category_path):
                file_path = os.path.join(category_path, file_name)
                # Ensure we're only adding image files
                if os.path.isfile(file_path) and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    my_list.append([file_path, category])
    return pd.DataFrame(my_list, columns=['file_path', 'label'])

# Dataset paths in Google Drive
dataset_dir = '/content/drive/My Drive/XAI_AA'  # Change this to your dataset path
train_dir = os.path.join(dataset_dir, 'train')
val_dir = os.path.join(dataset_dir, 'val')
test_dir = os.path.join(dataset_dir, 'test')

# Create DataFrames for train, validation, and test datasets
train_df = create_dataset(train_dir)
val_df = create_dataset(val_dir)
test_df = create_dataset(test_dir)

# Convert labels to numeric: NORMAL -> 0, PNEUMONIA -> 1
train_df['label'] = train_df['label'].map({'NORMAL': 0, 'PNEUMONIA': 1})
val_df['label'] = val_df['label'].map({'NORMAL': 0, 'PNEUMONIA': 1})
test_df['label'] = test_df['label'].map({'NORMAL': 0, 'PNEUMONIA': 1})

# Print dataset sizes
print(f"Train set size: {len(train_df)}, Validation set size: {len(val_df)}, Test set size: {len(test_df)}")

def count_categories(df, dataset_name):
    category_counts = df['label'].value_counts()
    print(f"{dataset_name} set:")
    print(f"  NORMAL: {category_counts.get(0, 0)}")
    print(f"  PNEUMONIA: {category_counts.get(1, 0)}")

# Count and display for train, validation, and test datasets
print("Image Counts per Category:")
count_categories(train_df, "Train")
count_categories(val_df, "Validation")
count_categories(test_df, "Test")

def visualize_samples(df, n_samples=5):
    fig, axes = plt.subplots(2, n_samples, figsize=(15, 10))

    # Visualize 5 NORMAL samples
    normal_samples = df[df['label'] == 0].iloc[:n_samples]
    for i in range(n_samples):
        img_path = normal_samples.iloc[i, 0]
        label = normal_samples.iloc[i, 1]
        image = Image.open(img_path).convert('L')  # Open in grayscale (original format)
        axes[0, i].imshow(image, cmap='gray')
        axes[0, i].set_title(f"Label: {['Normal', 'Pneumonia'][label]}")
        axes[0, i].axis('off')

    # Visualize 5 PNEUMONIA samples
    pneumonia_samples = df[df['label'] == 1].iloc[:n_samples]
    for i in range(n_samples):
        img_path = pneumonia_samples.iloc[i, 0]
        label = pneumonia_samples.iloc[i, 1]
        image = Image.open(img_path).convert('L')  # Open in grayscale (original format)
        axes[1, i].imshow(image, cmap='gray')
        axes[1, i].set_title(f"Label: {['Normal', 'Pneumonia'][label]}")
        axes[1, i].axis('off')

    plt.show()

# Show 5 samples from the training data
visualize_samples(train_df)

# Visualizing the class distribution
train_counts = train_df['label'].value_counts()
val_counts = val_df['label'].value_counts()
test_counts = test_df['label'].value_counts()

plt.figure(figsize=(8, 6))
plt.bar(['Normal', 'Pneumonia'], train_counts, color=['green','red'])
plt.title("Training Data Class Distribution")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(8, 6))
plt.bar(['Normal', 'Pneumonia'], val_counts, color=['green','red'])
plt.title("Validation Data Class Distribution")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(8, 6))
plt.bar(['Normal', 'Pneumonia'], test_counts, color=['green','red'])
plt.title("Test Data Class Distribution")
plt.ylabel("Count")
plt.show()

# Check for multi-GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Function to prepare model for multi-GPU usage
def prepare_model_for_multigpu(model):
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs!")
        model = nn.DataParallel(model)
    model = model.to(device)
    return model



class ImageDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx, 0]
        label = self.dataframe.iloc[idx, 1]
        img = Image.open(img_path).convert('RGB')

        if self.transform:
            img = self.transform(img)

        return img, label

# Define transformations for training and validation

# For Custom CNN (Grayscale Input)
train_transform_cnn = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(),  # Convert to grayscale (1 channel)
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(0.2),
    transforms.ToTensor(),

])

val_transform_cnn = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(),
    transforms.ToTensor(),
])

# For ResNet-18 (RGB Input)
"""train_transform_resnet = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB (3 channels)
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
])

val_transform_resnet = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB (3 channels)
    transforms.ToTensor(),
])"""

# Create datasets for both models
train_dataset_cnn = ImageDataset(train_df, transform=train_transform_cnn)
val_dataset_cnn = ImageDataset(val_df, transform=val_transform_cnn)
"""
train_dataset_resnet = ImageDataset(train_df, transform=train_transform_resnet)
val_dataset_resnet = ImageDataset(val_df, transform=val_transform_resnet)
"""
# DataLoader
batch_size = 32
train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=batch_size, shuffle=True)
val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=batch_size, shuffle=False)

"""train_loader_resnet = DataLoader(train_dataset_resnet, batch_size=batch_size, shuffle=True)
val_loader_resnet = DataLoader(val_dataset_resnet, batch_size=batch_size, shuffle=False)"""

# Define Custom CNN Model (from scratch)
class CustomCNN(nn.Module):
    def __init__(self):
        super(CustomCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1 input channel for grayscale
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(256 * 56 * 56, 128)
        self.fc2 = nn.Linear(128, 2)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)  # Pooling layer
        x = torch.relu(self.conv3(x))
        x = torch.relu(self.conv4(x))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model_from_scratch = CustomCNN().to(device)

model_from_scratch = prepare_model_for_multigpu(model_from_scratch)

def train_model_with_history(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        # Training loop
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # Calculate training loss and accuracy
        train_loss = running_loss / len(train_loader)
        train_accuracy = 100 * correct / total
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        # Calculate validation loss and accuracy
        val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * val_correct / val_total
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # Print results for the epoch
        print(f"Epoch [{epoch+1}/{num_epochs}] "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%")

    return train_losses, val_losses, train_accuracies, val_accuracies

# Define loss function and optimizer for Custom CNN
criterion = nn.CrossEntropyLoss()
optimizer_from_scratch = optim.Adam(model_from_scratch.parameters(), lr=0.001)

# Train the Custom CNN Model (from scratch)
print("Training Custom CNN Model (from scratch)...")
train_losses_cnn, val_losses_cnn, train_accuracies_cnn, val_accuracies_cnn = train_model_with_history(
    model_from_scratch, train_loader_cnn, val_loader_cnn, criterion, optimizer_from_scratch, num_epochs=10
)

def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, model_name):
    epochs = range(1, len(train_losses) + 1)

    # Plot Loss
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='Train Loss')
    plt.plot(epochs, val_losses, label='Val Loss')
    plt.title(f'{model_name} - Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, label='Train Accuracy')
    plt.plot(epochs, val_accuracies, label='Val Accuracy')
    plt.title(f'{model_name} - Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot results for Custom CNN (from scratch)
plot_training_history(train_losses_cnn, val_losses_cnn, train_accuracies_cnn, val_accuracies_cnn, "Custom CNN (from scratch)")

"""LOading predefined model"""

# Define optimizer for ResNet-18
optimizer_resnet = optim.Adam(resnet18.parameters(), lr=0.001)

# Train the Pretrained ResNet-18 Model
print("Training Pretrained ResNet-18 Model...")
train_losses_resnet, val_losses_resnet, train_accuracies_resnet, val_accuracies_resnet = train_model_with_history(
    resnet18, train_loader_resnet, val_loader_resnet, criterion, optimizer_resnet, num_epochs=10
)

# Plot training and validation loss/accuracy for ResNet-18
plot_training_history(train_losses_resnet, val_losses_resnet, train_accuracies_resnet, val_accuracies_resnet, "Pretrained ResNet-18")

# Create Dataset and DataLoader for Test Set (Custom CNN)
test_transform_cnn = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(),  # Keep image as grayscale (1 channel)
    transforms.ToTensor(),
])

test_dataset_cnn = ImageDataset(test_df, transform=test_transform_cnn)
test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32, shuffle=False)

#Create Dataset and DataLoader for Test Set (ResNet-18)
test_transform_resnet = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels (RGB)
    transforms.ToTensor(),
])

test_dataset_resnet = ImageDataset(test_df, transform=test_transform_resnet)
test_loader_resnet = DataLoader(test_dataset_resnet, batch_size=32, shuffle=False)

# Function to evaluate the model and return predictions, true labels, and scores
def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return all_labels, all_preds

# Evaluate Custom CNN from Scratch on Test Set
y_true_cnn, y_pred_cnn = evaluate_model(model_from_scratch, test_loader_cnn)

# Evaluate Pretrained ResNet18 on Test Set
#y_true_resnet, y_pred_resnet = evaluate_model(resnet18, test_loader_resnet)

# Print accuracy for both models
accuracy_cnn = accuracy_score(y_true_cnn, y_pred_cnn)
#accuracy_resnet = accuracy_score(y_true_resnet, y_pred_resnet)

print(f"Accuracy of Custom CNN from Scratch: {accuracy_cnn:.4f}")
#print(f"Accuracy of Pretrained ResNet18: {accuracy_resnet:.4f}")

# Function to plot confusion matrix with percentages
def plot_confusion_matrix_with_percentages(cm, model_name):
    plt.figure(figsize=(5, 5))
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    # Annotate with both raw numbers and percentages
    annot = np.empty_like(cm).astype(str)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            annot[i, j] = f"{cm[i, j]}\n({cm_percentage[i, j]:.1f}%)"

    sns.heatmap(cm, annot=annot, fmt="", cmap="Blues", xticklabels=["Normal", "Pneumonia"], yticklabels=["Normal", "Pneumonia"])
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

# Scores and Confusion Matrix for Custom CNN from Scratch
print("Custom CNN from Scratch:")
cm_cnn = confusion_matrix(y_true_cnn, y_pred_cnn)
plot_confusion_matrix_with_percentages(cm_cnn, "Custom CNN from Scratch")
print("Classification Report:\n", classification_report(y_true_cnn, y_pred_cnn))

# Scores and Confusion Matrix for Pretrained ResNet18
"""print("Pretrained ResNet18:")
cm_resnet = confusion_matrix(y_true_resnet, y_pred_resnet)
plot_confusion_matrix_with_percentages(cm_resnet, "Pretrained ResNet18")
print("Classification Report:\n", classification_report(y_true_resnet, y_pred_resnet))"""

def show_balanced_predictions(model, loader, class_names, num_normal=10, num_pneumonia=10):
    """
    Display images with balanced true labels (Normal and Pneumonia).
    Highlight incorrect predictions in red.
    """
    model.eval()  # Set the model to evaluation mode
    normal_images = []
    pneumonia_images = []

    # Gather predictions for "Normal" and "Pneumonia" separately
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predictions = torch.max(outputs, 1)

            for i in range(images.size(0)):
                # Convert image tensor to numpy array for display
                image = images[i].cpu().numpy().transpose(1, 2, 0)
                if image.shape[-1] == 1:  # Grayscale image
                    image = image.squeeze(-1)

                true_label = labels[i].item()
                pred_label = predictions[i].item()
                label_dict = {
                    "image": image,
                    "true_label": class_names[true_label],
                    "pred_label": class_names[pred_label],
                    "correct": true_label == pred_label,
                }

                # Collect images based on true labels
                if true_label == 0 and len(normal_images) < num_normal:
                    normal_images.append(label_dict)
                elif true_label == 1 and len(pneumonia_images) < num_pneumonia:
                    pneumonia_images.append(label_dict)

            # Break the loop if we have enough images
            if len(normal_images) >= num_normal and len(pneumonia_images) >= num_pneumonia:
                break

    # Combine images for display
    images_to_display = normal_images + pneumonia_images
    rows, cols = 4, 5  # Grid size
    fig, axes = plt.subplots(rows, cols, figsize=(15, 12))

    for idx, ax in enumerate(axes.flat):
        if idx < len(images_to_display):
            img_dict = images_to_display[idx]
            image = img_dict["image"]
            true_label = img_dict["true_label"]
            pred_label = img_dict["pred_label"]
            correct = img_dict["correct"]

            # Title with green for correct, red for incorrect
            title_color = "green" if correct else "red"
            ax.imshow(image, cmap="gray" if image.ndim == 2 else None)
            ax.set_title(f"True: {true_label}\nPred: {pred_label}", color=title_color)
            ax.axis("off")
        else:
            ax.axis("off")  # Empty cells

    plt.tight_layout()
    plt.show()

# Show predictions for Custom CNN
print("Balanced Predictions from Custom CNN:")
show_balanced_predictions(model_from_scratch, test_loader_cnn, class_names=["Normal", "Pneumonia"], num_normal=10, num_pneumonia=10)

# Show predictions for Pretrained ResNet18
#print("Balanced Predictions from Pretrained ResNet18:")
#show_balanced_predictions(resnet18, test_loader_resnet, class_names=["Normal", "Pneumonia"], num_normal=10, num_pneumonia=10)

!pip install captum

from captum.attr import IntegratedGradients
from captum.attr import visualization as viz
import matplotlib.pyplot as plt
import numpy as np

from captum.attr import IntegratedGradients
from captum.attr import visualization as viz
from captum.attr import LayerAttribution # Import LayerAttribution

import matplotlib.pyplot as plt
import numpy as np







from captum.attr import Occlusion
from captum.attr import visualization as viz
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage # Import ndimage for interpolation




occlusion = Occlusion(model_from_scratch)

# Define the sliding window size and stride
sliding_window_shapes = (1, 15, 15)  # Changed to 1 channel to match input image
strides = (1, 8, 8)  # Changed to 1 channel to match input image

# Calculate attributions
attr = occlusion.attribute(image.unsqueeze(0),
                            strides=strides,
                            sliding_window_shapes=sliding_window_shapes,
                            target=label,
                            baselines=0)

# Visualize the attributions
# Ensure attr has 3 dimensions before visualization
attr_np = attr.squeeze().cpu().detach().numpy()

# If attr_np is 2D, add a dimension for visualization
if attr_np.ndim == 2:
    attr_np = attr_np[:, :, np.newaxis]

# If image_np is grayscale (2D), add a dimension for visualization
image_np = image.squeeze().cpu().detach().numpy()
if image_np.ndim == 2:
    image_np = image_np[:, :, np.newaxis]

# Calculate the desired output shape based on original image size
output_shape = (image_np.shape[0] * 4, image_np.shape[1] * 4, image_np.shape[2])

# Perform interpolation to increase resolution
attr_np = ndimage.zoom(attr_np, (output_shape[0] / attr_np.shape[0],
                                   output_shape[1] / attr_np.shape[1],
                                   1), order=1)  # Upsample using zoom

image_np = ndimage.zoom(image_np, (output_shape[0] / image_np.shape[0],
                                   output_shape[1] / image_np.shape[1],
                                   1), order=1)  # Upsample image using zoom

viz.visualize_image_attr(attr_np,
                          image_np,
                          method="heat_map",
                          sign="all",  # Show both positive and negative attributions
                          cmap='RdBu_r', # Red for important, blue for less important
                          show_colorbar=True,
                          title="Occlusion")

from captum.attr import Occlusion
from captum.attr import visualization as viz
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage

def visualize_attributions_for_images(model, data_loader, num_images_per_class=5):
    """
    Visualizes attributions for multiple images of both classes and displays them.

    Args:
        model: The trained model.
        data_loader: The data loader for the dataset.
        num_images_per_class: Number of images to visualize per class.
    """
    occlusion = Occlusion(model)
    sliding_window_shapes = (1, 15, 15)
    strides = (1, 8, 8)

    normal_count = 0
    pneumonia_count = 0

    for image, label in data_loader:
        image, label = image[0].to(device), label[0].to(device)  # Assuming batch size 1

        if (label.item() == 0 and normal_count < num_images_per_class) or \
           (label.item() == 1 and pneumonia_count < num_images_per_class):

            attr = occlusion.attribute(image.unsqueeze(0),
                                        strides=strides,
                                        sliding_window_shapes=sliding_window_shapes,
                                        target=label,
                                        baselines=0)

            # Visualize the attributions
            attr_np = attr.squeeze().cpu().detach().numpy()
            if attr_np.ndim == 2:
                attr_np = attr_np[:, :, np.newaxis]

            image_np = image.squeeze().cpu().detach().numpy()
            if image_np.ndim == 2:
                image_np = image_np[:, :, np.newaxis]

            # Interpolation for smoother visualization
            output_shape = (image_np.shape[0] * 4, image_np.shape[1] * 4, image_np.shape[2])
            attr_np = ndimage.zoom(attr_np, (output_shape[0] / attr_np.shape[0],
                                           output_shape[1] / attr_np.shape[1],
                                           1), order=1)
            image_np = ndimage.zoom(image_np, (output_shape[0] / image_np.shape[0],
                                           output_shape[1] / image_np.shape[1],
                                           1), order=1)

            # Display the image and attribution
            fig, ax = plt.subplots()
            ax.imshow(image_np, cmap='gray')
            ax.imshow(attr_np, cmap='RdBu_r', alpha=0.5, extent=(0, image_np.shape[1], image_np.shape[0], 0))
            ax.set_title(f"{'Normal' if label.item() == 0 else 'Pneumonia'} - Occlusion")
            plt.show()

            if label.item() == 0:
                normal_count += 1
            else:
                pneumonia_count += 1

        if normal_count == num_images_per_class and pneumonia_count == num_images_per_class:
            break

# Call the function to visualize and display attributions
visualize_attributions_for_images(model_from_scratch, test_loader_cnn)